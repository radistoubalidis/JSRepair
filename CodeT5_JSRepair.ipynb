{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Pa7PA6UMH2pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "WWphmJa0ICOV",
        "outputId": "bc0ccb7f-8d4c-4946-d4d5-c239b3507ada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/radistoubalidis/JSRepair.git\n",
        "%cd"
      ],
      "metadata": {
        "id": "eOOMW_SjH4WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi-6we72Hti5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from transformers import (\n",
        "    RobertaTokenizer,\n",
        ")\n",
        "from modules.models import CodeT5\n",
        "from modules.datasets import CodeT5Dataset\n",
        "from modules.TrainConfig import init_logger, init_checkpoint, Trainer\n",
        "from modules.filters import add_labels\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import List\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9liNYnTHti6"
      },
      "outputs": [],
      "source": [
        "HF_DIR = 'Salesforce/codet5-small'\n",
        "TOKENIZER_MAX_LENGTH = 420 #int(input('Tokenizer Max length: '))\n",
        "DB_PATH = 'commitpack-datasets.db' if os.path.exists('commitpack-datasets.db') else ''\n",
        "DB_TABLE = 'commitpackft_classified_train'\n",
        "\n",
        "if not os.path.exists(DB_PATH):\n",
        "    raise RuntimeError('sqlite3 path doesnt exist.')\n",
        "VAL_SIZE = 0.3\n",
        "LOG_PATH = 'logs' if os.path.exists('logs') else ''\n",
        "MODEL_DIR = 'CodeT5JS'\n",
        "VERSION = 0#int(input('Training version: '))\n",
        "LOAD_FROM_CPKT = ''#input(\"Load from existing model (type cpkt path if true): \")\n",
        "DEBUG = True #if int(input('Debug Run (1,0): ')) == 1 else False\n",
        "BATCH_SIZE = 8 if DEBUG is True else 32\n",
        "CPKT_PATH = 'checkpoints'#input('Paste checkpoints dir: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urdA7KyTHti7"
      },
      "outputs": [],
      "source": [
        "con = sqlite3.connect(DB_PATH)\n",
        "\n",
        "with open('bug-type-dist-query.sql', 'r') as f:\n",
        "    query = f.read()\n",
        "\n",
        "info_df = pd.read_sql_query(query, con)\n",
        "info_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87tUL_bQHti7"
      },
      "source": [
        "# Create Classification Labels\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"mobile\" : 0,\n",
        "    \"functionality\" : 0,\n",
        "    \"ui-ux\" : 0,\n",
        "    \"compatibility-performance\" : 0,\n",
        "    \"network-security\" : 0,\n",
        "    \"general\": 0\n",
        "}\n",
        "```\n",
        "Ένα δείγμα που κατηγοριοποιήθηκε ως σφάλμα λειτουργικότητας(functionality) και ui-ux θα έχει διάνυσμα ταξινόμησης ->\n",
        "```[0,1,1,0,0,0]```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mRHGm6JHti8"
      },
      "outputs": [],
      "source": [
        "def load_ds() -> pd.DataFrame:\n",
        "    ds_df = pd.read_sql_query(f\"select * from {DB_TABLE}\", con)\n",
        "    return ds_df\n",
        "\n",
        "ds_df = load_ds()\n",
        "ds_df['class_labels'] = ds_df['bug_type'].apply(lambda bT: add_labels(bT))\n",
        "ds_df.head()\n",
        "old_codes = ds_df[['old_contents', 'class_labels']]\n",
        "new_codes = ds_df[['new_contents', 'class_labels']]\n",
        "\n",
        "TRAIN_old, VAL_old, TRAIN_new, VAL_new = train_test_split(old_codes, new_codes, test_size=VAL_SIZE, random_state=42)\n",
        "\n",
        "print(f\"Total training samples: {len(ds_df)}\")\n",
        "\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzy9IQ79Hti8"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Wwhjv1FHti8"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(HF_DIR)\n",
        "\n",
        "TRAIN_encodings = tokenizer(\n",
        "    TRAIN_old['old_contents'].tolist(),\n",
        "    max_length=TOKENIZER_MAX_LENGTH,\n",
        "    pad_to_max_length=True,\n",
        "    return_tensors='pt',\n",
        "    padding='max_length',\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "VAL_encodings = tokenizer(\n",
        "    VAL_old['old_contents'].tolist(),\n",
        "    max_length=TOKENIZER_MAX_LENGTH,\n",
        "    pad_to_max_length=True,\n",
        "    return_tensors='pt',\n",
        "    padding='max_length',\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "TRAIN_decodings = tokenizer(\n",
        "    TRAIN_new['new_contents'].tolist(),\n",
        "    max_length=TOKENIZER_MAX_LENGTH,\n",
        "    pad_to_max_length=True,\n",
        "    return_tensors='pt',\n",
        "    padding='max_length',\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "VAL_decodings = tokenizer(\n",
        "    VAL_new['new_contents'].tolist(),\n",
        "    max_length=TOKENIZER_MAX_LENGTH,\n",
        "    pad_to_max_length=True,\n",
        "    return_tensors='pt',\n",
        "    padding='max_length',\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnVOxTTIHti9"
      },
      "source": [
        "## Convert Class Labels into tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRoNjm0bHti9"
      },
      "outputs": [],
      "source": [
        "TRAIN_classes = list(\n",
        "    TRAIN_old['class_labels'].apply(\n",
        "        lambda labels: torch.tensor(labels)\n",
        "    )\n",
        ")\n",
        "\n",
        "VAL_classes = list(\n",
        "    VAL_old['class_labels'].apply(\n",
        "        lambda labels: torch.tensor(labels)\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a7NjypmHti9"
      },
      "outputs": [],
      "source": [
        "logger = init_logger(log_path=LOG_PATH, model_dir=MODEL_DIR, version=VERSION)\n",
        "checkpoint = init_checkpoint(cpkt_path=CPKT_PATH, model_dir=MODEL_DIR, version=VERSION)\n",
        "trainer = Trainer(checkpoint,logger,debug=DEBUG, num_epochs=3)\n",
        "\n",
        "if len(LOAD_FROM_CPKT) > 0 and  os.path.exists(LOAD_FROM_CPKT):\n",
        "    model = CodeT5.load_from_checkpoint(LOAD_FROM_CPKT)\n",
        "else:\n",
        "    model = CodeT5()\n",
        "model.model.train()\n",
        "model.model.to('cpu')\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_dataset = CodeT5Dataset(TRAIN_encodings, TRAIN_decodings, TRAIN_classes)\n",
        "VAL_dataset = CodeT5Dataset(VAL_encodings, VAL_decodings, VAL_classes)\n",
        "dataloader = DataLoader(TRAIN_dataset, batch_size=BATCH_SIZE,num_workers=7, shuffle=True)\n",
        "val_dataloader = DataLoader(VAL_dataset, batch_size=1, num_workers=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhQtadvkHti9"
      },
      "outputs": [],
      "source": [
        "trainer.fit(\n",
        "    model,\n",
        "    train_dataloaders=dataloader,\n",
        "    val_dataloaders=val_dataloader\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}