{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from transformers import RobertaTokenizer\n",
    "from modules.models import CodeBertJS,CodeT5\n",
    "from modules.datasets import CodeBertDataset, CodeT5Dataset\n",
    "from modules.metrics import CodeRouge\n",
    "from modules.filters import add_labels\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "SQLITE_PATH = 'commitpack-datasets.db' if os.path.exists('commitpack-datasets.db') else '/content/drive/MyDrive/Thesis/commitpack-datasets.db'\n",
    "CPKT_PATH = input('Paste cpkt path: ')\n",
    "if not os.path.exists(CPKT_PATH):\n",
    "    raise FileNotFoundError(f\"{CPKT_PATH} does not exist.\")\n",
    "os.environ['CPKT_PATH'] = CPKT_PATH\n",
    "MODEL_NAME = CPKT_PATH.split('/')[-1].split('.')[0].split('_')[0]\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME\n",
    "VERSION = CPKT_PATH.split('/')[-1].split('.')[0].split('_')[-1][-1]\n",
    "os.environ['VERSION'] = VERSION\n",
    "METRICS_PATH = 'metrics' if os.path.exists('metrics') else '/content/drive/MyDrive/Thesis/metrics'\n",
    "os.environ['METRICS_PATH'] = METRICS_PATH\n",
    "HF_DIR = 'Salesforce/codet5-small' if MODEL_NAME == 'CodeT5JS' else 'microsoft/codebert-base-mlm'\n",
    "MAX_LENGTH = 420 if MODEL_NAME == 'CodeT5JS' else 350\n",
    "DB_TABLE = \"commitpackft_classified_test\"\n",
    "con = sqlite3.connect(SQLITE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug Types Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bug-type-dist-query_test.sql', 'r')as f:\n",
    "    distQuery = f.read()\n",
    "f.close()\n",
    "info_df = pd.read_sql_query(distQuery, con)\n",
    "info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model And Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df = pd.read_sql_query(f\"select * from {DB_TABLE}\", con)\n",
    "ds_df['class_labels'] = ds_df['bug_type'].apply(lambda bT: add_labels(bT))\n",
    "ds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == 'CodeT5JS':\n",
    "    model = CodeT5.load_from_checkpoint(CPKT_PATH)\n",
    "elif 'CodeBertJS' in MODEL_NAME:\n",
    "    model = CodeBertJS.load_from_checkpoint(CPKT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_samples = model.tokenizer(\n",
    "    ds_df['old_contents'].tolist(),\n",
    "    max_length=MAX_LENGTH,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "encoded_labels = model.tokenizer(\n",
    "    ds_df['new_contents'].tolist(),\n",
    "    max_length=MAX_LENGTH,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "labels = torch.tensor(ds_df['class_labels'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == 'CodeT5JS':\n",
    "    torch_ds = CodeT5Dataset(encodings=encoded_samples, decodings=encoded_labels, class_labels=labels)\n",
    "elif 'CodeBertJS' in MODEL_NAME:\n",
    "    torch_ds = CodeBertDataset(encoded_samples, encoded_labels.input_ids)\n",
    "loader = DataLoader(torch_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer()\n",
    "\n",
    "trainer.test(model=model, dataloaders=loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = CodeRouge(['rouge7','rouge8','rouge9','rougeL','rougeLsum'])\n",
    "\n",
    "rouge.compute(predictions=model.generated_codes, references=ds_df['new_contents'].tolist())\n",
    "rouge.calc_averages()\n",
    "\n",
    "avgs_path = f\"{METRICS_PATH}/{MODEL_NAME}_v{VERSION}/rouge.json\"\n",
    "all_path = f\"{METRICS_PATH}/{MODEL_NAME}_v{VERSION}/avg_rouge.csv\"\n",
    "with open(avgs_path, 'w') as f:\n",
    "    json.dump(rouge.avgs, f, indent=4)\n",
    "\n",
    "all_scores = []\n",
    "for r in rouge.rouge_types:\n",
    "    all_scores += rouge.rouge_type_to_list(r)\n",
    "\n",
    "metrics_df = pd.DataFrame(all_scores)\n",
    "\n",
    "for m in ['precision','recall','fmeasure']:\n",
    "    metrics_df[m] = round(metrics_df[m], 3)\n",
    "metrics_df.to_csv(all_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "codebert_avgs = rouge.avgs\n",
    "\n",
    "comparison_model_path = input('Comparison model avg ROUGE-N metrics path: ')\n",
    "comparison_model = comparison_model_path.split('/')[-1].split('.')[0]\n",
    "if not os.path.exists(comparison_model_path):\n",
    "    raise RuntimeError('Metrics path does not exist.')\n",
    "\n",
    "with open(comparison_model_path, 'r') as f:\n",
    "    codet5_avgs = json.load(f)\n",
    "    \n",
    "\n",
    "plot_data = {\n",
    "    f\"{MODEL_NAME}_{VERSION}\": (round(codebert_avgs['avg_rouge7'].fmeasure, 5), round(codebert_avgs['avg_rouge8'].fmeasure, 5), round(codebert_avgs['avg_rouge9'].fmeasure, 5), round(codebert_avgs['avg_rougeL'].fmeasure, 5), round(codebert_avgs['avg_rougeLsum'].fmeasure, 5)),\n",
    "    comparison_model: (round(codet5_avgs['avg_rouge7'][2], 5), round(codet5_avgs['avg_rouge8'][2], 5), round(codet5_avgs['avg_rouge9'][2], 5), round(codet5_avgs['avg_rougeL'][2], 5), round(codet5_avgs['avg_rougeLsum'][2], 5)),\n",
    "}\n",
    "\n",
    "metric_types = ('Rouge-7', 'Rouge-8','Rouge-9', 'Rouge-L', 'Rouge-Lsum')\n",
    "x = np.arange(len(metric_types))\n",
    "width = 0.15\n",
    "multiplier = 0\n",
    "\n",
    "fix, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "\n",
    "for model, values in plot_data.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, values, width, label=model)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('F-Measure Model Comparison')\n",
    "ax.set_xticks(x + width, metric_types)\n",
    "ax.legend(loc='upper left', ncols=4)\n",
    "ax.set_ylim(0, 1.2)\n",
    "\n",
    "plt.savefig(f\"{METRICS_PATH}/{MODEL_NAME}_{VERSION}_vs_{comparison_model}_{BUG_TYPE}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "codebert_avgs = rouge.avgs  # Assuming rouge is a library/function that provides average scores\n",
    "\n",
    "comparison_model_path = input('Comparison model avg ROUGE-N metrics path: ')\n",
    "comparison_model = comparison_model_path.split('/')[-1].split('.')[0]\n",
    "if not os.path.exists(comparison_model_path):\n",
    "    raise RuntimeError('Metrics path does not exist.')\n",
    "\n",
    "with open(comparison_model_path, 'r') as f:\n",
    "    codet5_avgs = json.load(f)\n",
    "\n",
    "# Define metric types (assuming same metrics for both models)\n",
    "metric_types = ('Rouge-7', 'Rouge-8', 'Rouge-9', 'Rouge-L', 'Rouge-Lsum')\n",
    "\n",
    "# Create a figure with 3 rows (subplots) and 1 column\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20, 16))\n",
    "\n",
    "# Data dictionaries for each metric (assuming data structure from rouge)\n",
    "precision_data = {\n",
    "    f\"{MODEL_NAME}_{VERSION}\": (codebert_avgs['avg_rouge7'].precision, codebert_avgs['avg_rouge8'].precision, codebert_avgs['avg_rouge9'].precision, codebert_avgs['avg_rougeL'].precision, codebert_avgs['avg_rougeLsum'].precision),\n",
    "    comparison_model: (codet5_avgs['avg_rouge7'][0], codet5_avgs['avg_rouge8'][0], codet5_avgs['avg_rouge9'][0], codet5_avgs['avg_rougeL'][0], codet5_avgs['avg_rougeLsum'][0]),\n",
    "}\n",
    "recall_data = {\n",
    "    f\"{MODEL_NAME}_{VERSION}\": (codebert_avgs['avg_rouge7'].recall, codebert_avgs['avg_rouge8'].recall, codebert_avgs['avg_rouge9'].recall, codebert_avgs['avg_rougeL'].recall, codebert_avgs['avg_rougeLsum'].recall),\n",
    "    comparison_model: (codet5_avgs['avg_rouge7'][1], codet5_avgs['avg_rouge8'][1], codet5_avgs['avg_rouge9'][1], codet5_avgs['avg_rougeL'][1], codet5_avgs['avg_rougeLsum'][1]),\n",
    "}\n",
    "f1_data = {\n",
    "    f\"{MODEL_NAME}_{VERSION}\": (codebert_avgs['avg_rouge7'].fmeasure, codebert_avgs['avg_rouge8'].fmeasure, codebert_avgs['avg_rouge9'].fmeasure, codebert_avgs['avg_rougeL'].fmeasure, codebert_avgs['avg_rougeLsum'].fmeasure),\n",
    "    comparison_model: (round(codet5_avgs['avg_rouge7'][2], 5), round(codet5_avgs['avg_rouge8'][2], 5), round(codet5_avgs['avg_rouge9'][2], 5), round(codet5_avgs['avg_rougeL'][2], 5), round(codet5_avgs['avg_rougeLsum'][2], 5)),\n",
    "}\n",
    "\n",
    "\n",
    "# Plot Precision (ax1)\n",
    "for model, precision in precision_data.items():\n",
    "    ax1.plot(metric_types, precision, label=model, marker='s')  # 's' for square marker\n",
    "ax1.set_xlabel('ROUGE-N')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot Recall (ax2)\n",
    "for model, recall in recall_data.items():\n",
    "    ax2.plot(metric_types, recall, label=model, marker='s')  # 'o' for circle marker\n",
    "ax2.set_xlabel('ROUGE-N')\n",
    "ax2.set_ylabel('Recall')\n",
    "ax2.grid(True)\n",
    "\n",
    "# Plot F1 Score (ax3)\n",
    "for model, f1 in f1_data.items():\n",
    "    ax3.plot(metric_types, f1, label=model, marker='s')\n",
    "ax3.set_xlabel('ROUGE-N')\n",
    "ax3.set_ylabel('F-measure')\n",
    "ax3.grid(True)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the entire figure as a single PNG\n",
    "plt.savefig(f\"{METRICS_PATH}/{MODEL_NAME}_{VERSION}_vs_{comparison_model}_{BUG_TYPE}.png\", dpi=300, bbox_inches='tight')\n",
    "ax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
